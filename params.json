{"name":"ATP Smash","tagline":"","body":"# Table of Contents\r\n\r\n1. Process Write-up\r\n    * Overview and Motivation\r\n    * Data Clean-up\r\n    * Prediction\r\n2. Data Clean-up\r\n3. Feature Creation\r\n4. Prediction\r\n    * Some Exploratory Data Analysis\r\n    * Individual Logistic Regression (and other algorithms)\r\n    * Stacked Regression\r\n    \r\n5. Point by Point Simulator\r\n    * Link: \r\n    * Scraping\r\n    * Simulator Function\r\n    * Match Simulator\r\n    * Analysis of n-Match Simulator\r\n\r\n# Overview and Motivation\r\nSports prediction is a notoriously difficult task.  Whole architectures and betting regimes have developed for each sport, and tennis is no different.  The ATP ranking system for players is a very particular system that awards varying points to players according to their win/loss result in a tournament.  The amount of points depends upon the tournament type and what round the player lost in.  However, it is unclear whether this is a very good marker of whether one player will beat another particular player, since the points for each are fairly arbitrary.  Two of us are avid tennis players and we had some inspiration from fivethirtyeight.com which often analyzes the ratings and strengths of players in competitive games.  We felt there was thus a need to investigate alternative rating systems for tennis players, for both descriptive and predictive capacities.  How good are the rating systems at assessing player strengths?  What alternatives might be good for prediction?\r\n\r\nTo this end, we had two approaches.  We took a macro-level historical approach.  Based off of players' historical outcomes in matches against one another and their ratings at the time, we trained classification models off of match results.  This was a  micro-level approach whereby we used Markov models to simulate the evolution of a tennis match, point-by-point, given input characteristics of players and the surface they played on.\r\n\r\n\r\n\r\n\r\n## Data Clean-up\r\n\r\n### Macro Data Collection and Clean-up\r\nWe collected ATP match results from https://github.com/JeffSackmann/tennis_atp. \r\nFor the Match Results: Data clean-up for the match results required making sure that dataframes were properly constructed and concatenated.  This was made more difficult because CSV file had additional information on matches from different years (point by point data), and the way the matches were identified was different in both dataframes.  Therefore, we had to go through a complex process of ascertaining and creating columns that could be used to uniquely identify matches from both datasets in order to combine the datasets correctly.\r\n\r\nBefore delving into any predictive models, we sought to create some useful features to aid in prediction beyond those that were already contained.  This included some things like the recent and overall head to head score between the players and indicators for the kinds of players and matches involved.  We cleaned up the data and excluded matches where we were missing ranks for some of the players - in tennis, this is okay because there is very little chance for an unranked player to beat a ranked players, and many of those matches were also missing other information as well.\r\n\r\n### Micro Data Collection and Clean-up\r\nScraping information from atpworldtour.com, we assembled our own database with serve and return percentages of top 100 players by surface over the past 25 years. These stats included players’ first serve percentage, first serve points won, second serve points won, first serve return points won, and second serve return points won.\r\n\r\n## Delving into Prediction\r\n\r\n# Exploratory Data Analysis\r\n\r\nAfter constructing the columns for the Elo, Glicko, and Trueskill rating systems, we plotted the ratings of the top 5 players over their career so far. This better illustrates who is really the greatest of all time but showing peak rating and number of games played until each player's rating begins to stabilize.\r\n\r\n![Elo](https://raw.githubusercontent.com/dionhagan/atp-smash-final-project/master/images/top5elo.png?token=AJ7m-_OHuAS23QI9lKugbVhDKGn6_iq9ks5WdfmUwA%3D%3D)\r\n\r\n![Glicko](https://raw.githubusercontent.com/dionhagan/atp-smash-final-project/master/images/top5glicko.png?token=AJ7m-8fpzeadMyLL8GSGE2oycFE-yidRks5WdfmuwA%3D%3D)\r\n\r\n![Trueskill](https://raw.githubusercontent.com/dionhagan/atp-smash-final-project/master/images/top5ts.png?token=AJ7m-zRhfye1nVNsPH8rjveKzeOzjdeBks5WdguEwA%3D%3D)\r\n\r\n# Classification: Micro Level\r\n\r\nTo find the best fit model for predicting our data, we tried fitting several different types of classifiers to our data.\r\n\r\n## Baseline Model\r\n\r\nFor our baseline model, we considered how most uninformed parties would bet on the winner and decided that a model in which we always select the higher ranked player to win would be the best model to start with. Therefore, we planned to make our ultimate decision relative to this model. This baseline model resulted in around 65% accuracy, our number to beat.\r\n\r\n## Logistic Regression\r\n\r\nSince we want our model to predict a binary success for a given player, we decided to first try a Logistic Regression classifier. We fit 3 separate models on each pair's rating difference/mean for each rating system. These models all predicted the winner with around a 65-66% accuracy rate, which essentially performs at the same level as our baseline model.\r\n\r\nBecause these models did not individually predict much higher than our baseline model, we attempted to combine the results of each predictor by creating a stacked regression ensemble that regresses on the three classifiers as predictors. However, even as an ensemble it didn't predict any better than our baseline or logistic regression models.\r\n\r\n## Naive Bayes\r\n\r\nThe next model we fit was the Naive Bayes model; however, for this model we considered that we should perhaps include all of the features and engineer some more to increase the model's predictive power. We added features to keep track of a players head to head win percentage/count as well as whether or not the player was in the top 10 (better players)\r\n\r\n## Adaboost\r\n\r\nWhile the additional features didn't help improve the accuracy in the Naive Bayes model, the ensemble Adaboost classifier experienced a dramatic increase in accuracy, predictting 92% correctly on the test set.\r\n\r\n# Graphic Analysis\r\n\r\n## ROC\r\n\r\nFrom the ROC graph we can see that the Adaboost has a much greater area under the curve than the competing classifiers. This indicates to us, along with the improved accuracy, that this is a much better model than the previous models we constructed.\r\n\r\n## Relative Feature Importance\r\n\r\nSince we're ultimately interested in the factors of a player that make him more likely to win a given match, we also took a look at the importance of each individual feature relative to the others in the Adaboost classifier model. As illustrated in the graph below, the rating systems were the strongest predictive factors in the model with the Glicko rating system proving far more useful of a predictor than the rest. Because many of the predictors in our models were fairly uninformative, we can shed more light on why our original results were so close to the baseline model. We just didn't have enough data to properly learn the models in those cases.\r\n\r\n### Classification: Macro Level\r\n\r\n#### Key issues\r\nThe macro-level approach essentially used match-level data for predictions.  However, this problem is actually quite tricky due to the way that pairwise competitive matches are encoded.  We wrestled with this for quite a while - essentially, what is the right way to encode the response variable, given that we know who the winners and losers are for each match?  A full discussion can be seen here: http://stats.stackexchange.com/questions/11800/how-should-we-convert-sports-results-data-to-perform-a-valid-logistical-regressi.  Tennis lacks an intuitive way for doing this (for example, home team vs. away team), so we ultimately opted to go with a duplication of the dataset with the exactly reversed outcomes and statistics.  This provides the symmetry and balance negative classes necessary for the logistic regression to perform accurately.\r\n\r\nWe then created some key metrics of prediction, notably implementing the elo, glicko, and trueskill rating systems.  These rating systems have the same goal of representing players’ relative difference in skill and lend us a significant degree of predictive power when forecasting a match.  However, it is very important to encode these systems correctly, particularly given the way the response variable was encoded.  This was discussed with professor who does sports prediction.  Therefore, rather than reporting the raw ratings outputted by each system, instead we used the relative ratings between the players.  These relative ratings are contained in two variables - the mean of the two ratings and the difference between the two ratings.  This encoding method is very much consistent with the idea of symmetry as created by the duplication of a reversed dataset.\r\n#### Prediction Paradigm\r\nWe used two paradigms for prediction: cross-validation and \"holdout\" validation.  For cross-validation, we experimented with separately using each rating system as a predictor, as well as using combinations of them.  In the \"holdout\" paradigm, we split the data into explicit training/validation/test sets.  We fit each rating model separately and then made an ensemble of their prediction results to see if we could generate any additional predictive power.\r\n\r\n### Classification: Results\r\nUsing the head to head, ranking, elo, glicko, trueskill - TODO\r\n\r\n![ROC](https://raw.githubusercontent.com/dionhagan/atp-smash-final-project/master/images/roc.png?token=AJ7m-5wLjatekBZJjMxPw0iO3MQywDvsks5WdfimwA%3D%3D)\r\n\r\n![Feature Importance] (https://raw.githubusercontent.com/dionhagan/atp-smash-final-project/master/images/featureimportance.png?token=AJ7m-2lFQzIHjBSU-azuOya5OZJEtNLNks5Wdfk0wA%3D%3D)\r\n\r\n\r\n### Markov Model: Micro Level\r\n\r\n\r\nThe micro-level approach essentially used the internal statistics for particular players to create a generative model for the evolution of a particular match.  This emphasized the serve, return and surface statistics, since these are well-known to highly affect outcomes.  These were in-match statistics that we couldn't take advantage of in the macro-level approach, and we wanted to exhaust the useful information within and beyond our datasets.\r\n\r\nIn tennis there are three general types of courts: clay, hard, and grass (we have left out several more specialized types of courts for simplicity, such as indoor and carpet). Many pro players exhibit differences in performance across these three surfaces. Historically, aggressive players have performed better on hard and grass courts, as the ball travels faster off the bounce, while defensive players prefer the extra recovery time allotted by clay courts.\r\n\r\n![nadal](http://cdn.theatlantic.com/static/mt/assets/culture_test/nadal%20courts%20craft%20apimages%20615.png)\r\n \r\n \r\nIn tennis, players alternate games as server. Within each game, they are allowed up to two serves per point. By normalizing players’ win percentages on first and second serves with their opponents’ win percentages on first and second serve returns, we created new overall percentages for players to win points on their serves. Given our dataset, these normalized percentages are meant to reflect players’ relative effectiveness as server and returner on a given surface in a given year.\r\n \r\nTo begin the match, our simulator flips a coin and chooses to let one of the two players serve. Then, with respect to tennis’ scoring system, the match progresses point after point, until a player wins two out of three sets.\r\n\r\n### Markov Model: Results\r\nTo explore players’ relative effectiveness on different court surfaces, we may run thousands of match simulations, keeping track of the winner in each. For example, take the matchup between world no. 1 Novak Djokovic and world no. 5 David Ferrer in 2011. While Djokovic is clearly a better player, Ferrer certainly possesses an affinity for clay courts. When we run 10,000 match simulations on clay, Djokovic wins just over 50% of them. When we run 10,000 matches on grass, however, Djokovic seems to come out on top around 70% of the time. From this simulator, this example being one among many, we see the great effect that court surface may have upon the outcome of a match.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}