{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview and Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sports prediction is a notoriously difficult task.  Whole architectures and betting regimes have developed for each sport, and tennis is no different.  The ATP ranking system for players is a very particular system that awards varying points to players according to their win/loss result in a tournament.  The amount of points depends upon the tournament type and what round the player lost in.  However, it is unclear whether this is a very good marker of whether one player will beat another particular player, since the points for each are fairly arbitrary.  We had some inspiration from fivethirtyeight.com which often analyzes the ratings and strengths of players in competitive games.  We felt there was thus a need to investigate alternative rating systems for tennis players, for both descriptive and predictive capacities.\n",
    "\n",
    "To this end, we had two approaches.  We took a macro-level historical approach.  Based off of players' historical outcomes in matches against one another and their ratings at the time, we trained classification models off of match results.  We could not break through about 66% accuracy, so we decided to pursue an additional direction.  This was a  micro-level approach whereby we used Markov models to simulate the evolution of a tennis match, point-by-point, given input characteristics of players and the surface they played on.\n",
    "\n",
    "### Data Clean-up\n",
    "\n",
    "We collected ATP match results from https://github.com/JeffSackmann/tennis_atp and scraped from ___________.  Data clean-up for the match results required making sure that dataframes were properly constructed and concatenated.  This was made more difficult because CSV file had additional information on matches from different years (point by point data), and the way the matches were identified was different in both dataframes.  Therefore, we had to go through a complex process of ascertaining and creating columns that could be used to uniquely identify matches from both datasets in order to combine the datasets correctly.\n",
    "\n",
    "Before delving into any predictive models, we sought to create some useful features to aid in prediction beyond those that were already contained.  This included some things like the recent and overall head to head score between the players and indicators for the kinds of players and matches involved.  We cleaned up the data and excluded matches where we were missing ranks for some of the players - in tennis, this is okay because there is very little chance for an unranked player to beat a ranked players, and many of those matches were also missing other information as well.\n",
    "\n",
    "### Delving into Prediction\n",
    "\n",
    "#### Macro Level: Classification\n",
    "The macro-level approach essentially used match-level data for predictions.  However, this problem is actually quite tricky due to the way that pairwise competitive matches are encoded.  We wrestled with this for quite a while - essentially, what is the right way to encode the response variable, given that we know who the winners and losers are for each match?  A full discussion can be seen here: http://stats.stackexchange.com/questions/11800/how-should-we-convert-sports-results-data-to-perform-a-valid-logistical-regressi.  Tennis lacks an intuitive way for doing this (for example, home team vs. away team), so we ultimately opted to go with a duplication of the dataset with the exactly reversed outcomes and statistics.  This provides the symmetry and balance negative classes necessary for the logistic regression to perform accurately.\n",
    "\n",
    "We then created some key metrics of prediction, notably implementing the elo, glicko, and trueskill rating systems.  These rating systems have the same goal of representing players’ relative difference in skill and lend us a significant degree of predictive power when forecasting a match.  However, it is very important to encode these systems correctly, particularly given the way the response variable was encoded.  This was discussed with professor who does sports prediction.  Therefore, rather than reporting the raw ratings outputted by each system, instead we used the relative ratings between the players.  These relative ratings are contained in two variables - the mean of the two ratings and the difference between the two ratings.  This encoding method is very much consistent with the idea of symmetry as created by the duplication of a reversed dataset.\n",
    "\n",
    "We used two paradigms for prediction: cross-validation and \"holdout\" validation.  For cross-validation, we experimented with separately using each rating system as a predictor, as well as using combinations of them.  In the \"holdout\" paradigm, we split the data into explicit training/validation/test sets.  We fit each rating model separately and then made an ensemble of their prediction results to see if we could generate any additional predictive power.\n",
    "\n",
    "#### Micro Level: Follows a Markov model\n",
    "The micro-level approach essentially used the internal statistics for particular players to create a generative model for the evolution of a particular match.  This emphasized the serve, return and surface statistics, since these are well-known to highly affect outcomes.  These were in-match statistics that we couldn't take advantage of in the macro-level approach, and we wanted to exhaust the useful information within and beyond our datasets.\n",
    "\n",
    "In tennis there are three general types of courts: clay, hard, and grass (we have left out several more specialized types of courts for simplicity, such as indoor and carpet). Many pro players exhibit differences in performance across these three surfaces. Historically, aggressive players have performed better on hard and grass courts, as the ball travels faster off the bounce, while defensive players prefer the extra recovery time allotted by clay courts.\n",
    "\n",
    "(photo link: http://cdn.theatlantic.com/static/mt/assets/culture_test/nadal%20courts%20craft%20apimages%20615.png)\n",
    " \n",
    "Scraping information from atpworldtour.com, we assembled our own database with serve and return percentages of top 100 players by surface over the past 25 years. These stats included players’ first serve percentage, first serve points won, second serve points won, first serve return points won, and second serve return points won.\n",
    " \n",
    "In tennis, players alternate games as server. Within each game, they are allowed up to two serves per point. By normalizing players’ win percentages on first and second serves with their opponents’ win percentages on first and second serve returns, we created new overall percentages for players to win points on their serves. Given our dataset, these normalized percentages are meant to reflect players’ relative effectiveness as server and returner on a given surface in a given year.\n",
    " \n",
    "To begin the match, our simulator flips a coin and chooses to let one of the two players serve. Then, with respect to tennis’ scoring system, the match progresses point after point, until a player wins two out of three sets.\n",
    " \n",
    "To explore players’ relative effectiveness on different court surfaces, we may run thousands of match simulations, keeping track of the winner in each. For example, take the matchup between world no. 1 Novak Djokovic and world no. 5 David Ferrer in 2011. While Djokovic is clearly a better player, Ferrer certainly possesses an affinity for clay courts. When we run 10,000 match simulations on clay, Djokovic wins just over 50% of them. When we run 10,000 matches on grass, however, Djokovic seems to come out on top around 70% of the time. From this simulator, this example being one among many, we see the great effect that court surface may have upon the outcome of a match.\n",
    "\n",
    "\n",
    "Project Scope - Did you choose the appropriate complexity and level of difficulty of your project?\n",
    "\n",
    "Process Book - Did you follow the data science process and is it well documented in your process book?\n",
    "\n",
    "Solution - Is your analysis effective and correct in answering your intended questions?\n",
    "\n",
    "Implementation - What is the quality of your code? Is it appropriately polished, robust, and reliable?\n",
    "\n",
    "Presentation - Are your web site and screencast clear, engaging, and effective?\n",
    "\n",
    "Peer Evaluations - Your individual project score will also be influenced by your peer evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview and Motivation: Provide an overview of the project goals and the motivation for it. Consider that this will be read by people who did not see your project proposal.\n",
    "\n",
    "Related Work: Anything that inspired you, such as a paper, a web site, or something we discussed in class.\n",
    "\n",
    "Initial Questions: What questions are you trying to answer? How did these questions evolve over the course of the project? What new questions did you consider in the course of your analysis? - Data: Source, scraping method, cleanup, storage, etc.\n",
    "\n",
    "Exploratory Data Analysis: What visualizations did you use to look at your data in different ways? What are the different statistical methods you considered? Justify the decisions you made, and show any major changes to your ideas. How did you reach these conclusions?\n",
    "\n",
    "Final Analysis: What did you learn about the data? How did you answer the questions? How can you justify your answers?\n",
    "\n",
    "Presentation: Present your final results in a compelling and engaging way using text, visualizations, images, and videos on your project web site.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dionhagan/anaconda/lib/python2.7/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n"
     ]
    }
   ],
   "source": [
    "# Install 3 libraries for rating systems\n",
    "# easy_install trueskill\n",
    "# !pip install git+https://github.com/sublee/elo\n",
    "# !pip install git+https://github.com/sublee/glicko2\n",
    "\n",
    "# Imports necessary libraries\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn import linear_model\n",
    "from operator import itemgetter, attrgetter\n",
    "import elo\n",
    "import glicko2\n",
    "import trueskill\n",
    "import matplotlib\n",
    "#matplotlib.use('TkAgg')\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53054, 50)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "Combines CSV 1999-2015 complete match files and constructs 1 dataframe.\n",
    "Choose 1999 as starting year because that's when Roger Federer starts playing.\n",
    "'''\n",
    "atp_year_list = []\n",
    "for i in xrange(1999,2016):\n",
    "    atp_year_list.append(pd.read_csv(\"./tennis_data/atp_matches_{0}.csv\".format(i)))\n",
    "atp_all_matches = pd.concat(atp_year_list, ignore_index = True)\n",
    "\n",
    "# Set up the pbp (point by point) column that will contain the pbp information for matches we have info on\n",
    "atp_all_matches['pbp'] = [None]*atp_all_matches.shape[0]\n",
    "atp_all_matches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dionhagan/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:18: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>tourney_id</th>\n",
       "      <th>tourney_name</th>\n",
       "      <th>surface</th>\n",
       "      <th>draw_size</th>\n",
       "      <th>tourney_level</th>\n",
       "      <th>tourney_date</th>\n",
       "      <th>match_num</th>\n",
       "      <th>winner_id</th>\n",
       "      <th>winner_seed</th>\n",
       "      <th>...</th>\n",
       "      <th>l_1stWon</th>\n",
       "      <th>l_2ndWon</th>\n",
       "      <th>l_SvGms</th>\n",
       "      <th>l_bpSaved</th>\n",
       "      <th>l_bpFaced</th>\n",
       "      <th>pbp</th>\n",
       "      <th>tourney_date1</th>\n",
       "      <th>match_year</th>\n",
       "      <th>match_month</th>\n",
       "      <th>score_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2954</td>\n",
       "      <td>1999-451</td>\n",
       "      <td>Doha</td>\n",
       "      <td>Hard</td>\n",
       "      <td>32</td>\n",
       "      <td>A</td>\n",
       "      <td>19990104</td>\n",
       "      <td>20</td>\n",
       "      <td>102223</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>19</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>None</td>\n",
       "      <td>1999-01-04</td>\n",
       "      <td>1999</td>\n",
       "      <td>1</td>\n",
       "      <td>6-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2937</td>\n",
       "      <td>1999-451</td>\n",
       "      <td>Doha</td>\n",
       "      <td>Hard</td>\n",
       "      <td>32</td>\n",
       "      <td>A</td>\n",
       "      <td>19990104</td>\n",
       "      <td>3</td>\n",
       "      <td>102950</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>None</td>\n",
       "      <td>1999-01-04</td>\n",
       "      <td>1999</td>\n",
       "      <td>1</td>\n",
       "      <td>6-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2938</td>\n",
       "      <td>1999-451</td>\n",
       "      <td>Doha</td>\n",
       "      <td>Hard</td>\n",
       "      <td>32</td>\n",
       "      <td>A</td>\n",
       "      <td>19990104</td>\n",
       "      <td>4</td>\n",
       "      <td>101543</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>None</td>\n",
       "      <td>1999-01-04</td>\n",
       "      <td>1999</td>\n",
       "      <td>1</td>\n",
       "      <td>6-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2939</td>\n",
       "      <td>1999-451</td>\n",
       "      <td>Doha</td>\n",
       "      <td>Hard</td>\n",
       "      <td>32</td>\n",
       "      <td>A</td>\n",
       "      <td>19990104</td>\n",
       "      <td>5</td>\n",
       "      <td>102338</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>17</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>None</td>\n",
       "      <td>1999-01-04</td>\n",
       "      <td>1999</td>\n",
       "      <td>1</td>\n",
       "      <td>2-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2940</td>\n",
       "      <td>1999-451</td>\n",
       "      <td>Doha</td>\n",
       "      <td>Hard</td>\n",
       "      <td>32</td>\n",
       "      <td>A</td>\n",
       "      <td>19990104</td>\n",
       "      <td>6</td>\n",
       "      <td>102271</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>30</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>None</td>\n",
       "      <td>1999-01-04</td>\n",
       "      <td>1999</td>\n",
       "      <td>1</td>\n",
       "      <td>1-6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index tourney_id tourney_name surface  draw_size tourney_level  \\\n",
       "0   2954   1999-451         Doha    Hard         32             A   \n",
       "1   2937   1999-451         Doha    Hard         32             A   \n",
       "2   2938   1999-451         Doha    Hard         32             A   \n",
       "3   2939   1999-451         Doha    Hard         32             A   \n",
       "4   2940   1999-451         Doha    Hard         32             A   \n",
       "\n",
       "   tourney_date  match_num  winner_id  winner_seed     ...      l_1stWon  \\\n",
       "0      19990104         20     102223          NaN     ...            38   \n",
       "1      19990104          3     102950          NaN     ...            16   \n",
       "2      19990104          4     101543          NaN     ...            22   \n",
       "3      19990104          5     102338            3     ...            38   \n",
       "4      19990104          6     102271          NaN     ...            31   \n",
       "\n",
       "  l_2ndWon l_SvGms  l_bpSaved l_bpFaced   pbp  tourney_date1  match_year  \\\n",
       "0       19      16          2         7  None     1999-01-04        1999   \n",
       "1        7       7          2         6  None     1999-01-04        1999   \n",
       "2       10       9          4         7  None     1999-01-04        1999   \n",
       "3       17      13         11        16  None     1999-01-04        1999   \n",
       "4       30      14         11        16  None     1999-01-04        1999   \n",
       "\n",
       "   match_month  score_start  \n",
       "0            1          6-2  \n",
       "1            1          6-1  \n",
       "2            1          6-3  \n",
       "3            1          2-6  \n",
       "4            1          1-6  \n",
       "\n",
       "[5 rows x 55 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "More dataframe preparation for combining dateframes.\n",
    "Matches detailed match info for select matches to correct row in complete match list\n",
    "If date is within 30 days of each other is correct, players are correct,\n",
    "result is same, and first set score is same, then it's the same match.\n",
    "'''\n",
    "\n",
    "# Get score into the same string format.  Some scores are mis-recorded,\n",
    "# so that's why I don't match on the entire score string\n",
    "atp_all_matches['score'] = atp_all_matches['score'].astype(str)\n",
    "\n",
    "# Get dates into the same format\n",
    "atp_all_matches['tourney_date'].apply(lambda x: (str(x)[8:16]))\n",
    "atp_all_matches['tourney_date1'] = atp_all_matches['tourney_date'].apply(lambda x: datetime.datetime.strptime(str(x), \"%Y%m%d\"))\n",
    "atp_all_matches['match_year'] = atp_all_matches['tourney_date1'].apply(lambda x: x.year)\n",
    "atp_all_matches['match_month'] = atp_all_matches['tourney_date1'].apply(lambda x: x.month)\n",
    "atp_all_matches['score_start'] = atp_all_matches['score'].apply(lambda x: x[:3])\n",
    "atp_all_matches = atp_all_matches.sort(['tourney_date1'], ascending=1).reset_index()\n",
    "atp_all_matches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Combine all the matches that have pbp (point by point) information into one dataframe\n",
    "and clean up columns in preparation for matching with the all_atp_matches dataframe.\n",
    "'''\n",
    "\n",
    "pbp_matches_archive = pd.read_csv(\"./tennis_data/pbp_matches_atp_main_archive.csv\")\n",
    "pbp_matches_current = pd.read_csv(\"./tennis_data/pbp_matches_atp_main_current.csv\")\n",
    "pbp_matches = pd.concat([pbp_matches_archive,pbp_matches_current])\n",
    "pbp_matches.winner = pbp_matches.winner - 1\n",
    "pbp_matches['winner_name'] = np.where(pbp_matches['winner'] == 0, pbp_matches['server1'], pbp_matches['server2'])\n",
    "pbp_matches['loser_name'] = np.where(pbp_matches['winner'] == 0, pbp_matches['server2'], pbp_matches['server1'])\n",
    "pbp_matches['date'] = pd.to_datetime(pbp_matches['date'])\n",
    "pbp_matches['match_year'] = pbp_matches['date'].apply(lambda x: x.year)\n",
    "pbp_matches['match_month'] = pbp_matches['date'].apply(lambda x: x.month)\n",
    "pbp_matches['score_start'] = pbp_matches['score'].apply(lambda x: x[:3])\n",
    "\n",
    "# Makes pbp_matches dataframe only contain the matching columns and the new column we want \"pbp\"\n",
    "pbp_matches = pbp_matches.iloc[:,[7,10,11,12,13,14]]\n",
    "\n",
    "''' \n",
    "Here, we actually add the 'pbp' column to the correct row in the other dataframe,\n",
    "based on criteria I chose that should be the same match,\n",
    "ie. correct winner and loser, same first set score, and match dates\n",
    "are within 30 days of one another.\n",
    "'''\n",
    "pbp_matches1 = pd.merge(atp_all_matches, pbp_matches, \\\n",
    "                        on=['winner_name', 'loser_name', 'match_year', 'score_start', 'match_month'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for Adding in Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "''' \n",
    "Returns 2 lists of head to head scores to be concatenated to the dataframe.\n",
    "It's a complicated function because you need to look at the histories of the players\n",
    "before adding the value for that row.\n",
    "'''\n",
    "def h2h(match_dataframe):\n",
    "    matches = match_dataframe\n",
    "    h2h_list_all, h2h_list_recent  = [], []\n",
    "    for i, row in matches.iterrows():\n",
    "        h2h_matches = matches[((matches['w_name'] == row['w_name']) & (matches['l_name'] == row['l_name'])) | \\\n",
    "                          ((matches['w_name'] == row['l_name']) & (matches['l_name'] == row['w_name']))]\n",
    "\n",
    "        match_date = pd.Timestamp(row['tourney_date1']).to_pydatetime()\n",
    "        one_year_date = match_date + datetime.timedelta(weeks=-52)\n",
    "        begin_date = datetime.datetime.strptime('1999-01-01', '%Y-%m-%d')\n",
    "        \n",
    "        h2h_all_prev_matches = h2h_matches[(h2h_matches.tourney_date1 >= begin_date) & (h2h_matches.tourney_date1 < match_date)]\n",
    "        h2h_recent_matches = h2h_matches[(h2h_matches.tourney_date1 >= one_year_date) & (h2h_matches.tourney_date1 < match_date)]\n",
    "\n",
    "        player1_perspective_all = (sum(h2h_all_prev_matches['w_name'] == row['w_name']),sum(h2h_all_prev_matches['w_name'] == row['l_name']))\n",
    "        player1_perspective_rec = (sum(h2h_recent_matches['w_name'] == row['w_name']),sum(h2h_recent_matches['w_name'] == row['l_name']))\n",
    "        h2h_list_all.append(player1_perspective_all)\n",
    "        h2h_list_recent.append(player1_perspective_rec)\n",
    "    return h2h_list_all, h2h_list_recent\n",
    "\n",
    "'''\n",
    "Indicator for top10\n",
    "'''     \n",
    "def top10_player(rank):\n",
    "    if rank < 10:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "'''\n",
    "Functions for head to head calculations\n",
    "'''\n",
    "def h2h_diff(tup):\n",
    "    x, y = tup\n",
    "    return x - y\n",
    "\n",
    "def h2h_count(tup):\n",
    "    x, y = tup\n",
    "    return x+y\n",
    "\n",
    "def h2h_prop1(tup):\n",
    "    if h2h_count(tup) != 0:\n",
    "        x, y = tup\n",
    "        return x/(x+y)\n",
    "    return 0\n",
    "\n",
    "def h2h_prop2(tup):\n",
    "    if h2h_count(tup) != 0:\n",
    "        x, y = tup\n",
    "        return y/(x+y)\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add columns to dataframe, a bit of handling missing NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'p1_rank'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5cee2d80a24c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0matp_all_matches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'player1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matp_all_matches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0matp_all_matches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'player2'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matp_all_matches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0matp_all_matches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'p1_top10'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matp_all_matches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp1_rank\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop10_player\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0matp_all_matches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'p2_top10'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matp_all_matches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp2_rank\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop10_player\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dionhagan/anaconda/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2244\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2245\u001b[0m             raise AttributeError(\"'%s' object has no attribute '%s'\" %\n\u001b[0;32m-> 2246\u001b[0;31m                                  (type(self).__name__, name))\n\u001b[0m\u001b[1;32m   2247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'p1_rank'"
     ]
    }
   ],
   "source": [
    "# Shorten column names\n",
    "atp_all_matches.columns = [x.replace('winner', 'w') for x in atp_all_matches.columns]\n",
    "atp_all_matches.columns = [x.replace('loser', 'l') for x in atp_all_matches.columns]\n",
    "\n",
    "# Set up player1 and player2 dynamic instead of winner/loser\n",
    "p1_columns = ['p1_rank', 'p1_seed','p1_rank_points','p1_ace', 'p1_df', 'p1_svpt', 'p1_1stWon', \\\n",
    "              'p1_2ndWon', 'p1_SvGms', 'p1_bpSaved', 'p1_bpFaced']\n",
    "p2_columns = [x.replace('p1','p2') for x in p1_columns]\n",
    "pw_columns = [x.replace('p1','w') for x in p1_columns]\n",
    "pl_columns = [x.replace('p1','l') for x in p1_columns]\n",
    "\n",
    "# Remove matches with missing ranking for either player\n",
    "old_all_atp_matches_nothing_removed = atp_all_matches\n",
    "atp_all_matches = atp_all_matches.drop(atp_all_matches[np.isnan(atp_all_matches.w_rank) | np.isnan(atp_all_matches.l_rank)].index)\n",
    "\n",
    "# What kind of Grandslam is it?\n",
    "atp_all_matches['grandslam_type'] = np.where(atp_all_matches['tourney_name'] == \\\n",
    "                                        ('US Open' or 'Wimbledon' or 'Australian Open' or 'Roland Garros'), \\\n",
    "                                        atp_all_matches['tourney_name'],'No') \n",
    "# Is it a Davis Cup Match?\n",
    "atp_all_matches['is_davis_cup'] = np.where(atp_all_matches['tourney_name'].str.contains('Davis Cup'), 'Yes', 'No')\n",
    "\n",
    "# Add in rank columns\n",
    "atp_all_matches['mean_rank'] = atp_all_matches.apply(lambda x: ((x['w_rank'] + x['l_rank']) / 2), axis = 1)\n",
    "atp_all_matches['diff_rank'] = atp_all_matches.apply(lambda x: x['w_rank'] - x['l_rank'], axis = 1)\n",
    "\n",
    "# Add in names and indicators\n",
    "atp_all_matches['player1'] = atp_all_matches.w_name\n",
    "atp_all_matches['player2'] = atp_all_matches.l_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adds in Head to Head columns from Winner's POV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create Head to Head Series for adding\n",
    "a, b = h2h(atp_all_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create columns with Head to Head information\n",
    "atp_all_matches['h2h_all'] = pd.Series(a, index=atp_all_matches.index)\n",
    "atp_all_matches['h2h_recent'] =  pd.Series(b, index=atp_all_matches.index)\n",
    "\n",
    "# Create columns with Head to Head differences/means\n",
    "atp_all_matches[\"h2h_all_diff\"] = atp_all_matches.h2h_all.apply(h2h_diff)\n",
    "atp_all_matches[\"h2h_all_count\"] = atp_all_matches.h2h_all.apply(h2h_count)\n",
    "atp_all_matches[\"p1_h2h_prop\"] = atp_all_matches.h2h_all.apply(h2h_prop1)\n",
    "atp_all_matches[\"p2_h2h_prop\"] = atp_all_matches.h2h_all.apply(h2h_prop2)\n",
    "atp_all_matches['p1_top10'] = atp_all_matches.p1_rank.apply(top10_player)\n",
    "atp_all_matches['p2_top10'] = atp_all_matches.p2_rank.apply(top10_player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### DO NOT DELETE ####\n",
    "\n",
    "#### RANDOMLY CHOOSE PLAYER 1 - 50/50 Split ####\n",
    "#atp_all_matches['player1'] = atp_all_matches.apply(lambda x: np.random.choice([x['w_name'],x['l_name']],1, replace = False)[0], axis = 1)\n",
    "#atp_all_matches['player2'] = atp_all_matches.apply(lambda x: x['w_name'] if x['player1'] == x['l_name'] else x['l_name'], axis = 1)\n",
    "\n",
    "#### CHOOSE PLAYER 1 BASED ON BETTER RANK ####\n",
    "#atp_all_matches['player1'] = atp_all_matches.apply(lambda x: x['w_name'] if x['l_rank'] > x['w_rank'] else x['l_name'], axis = 1)\n",
    "#atp_all_matches['player2'] = atp_all_matches.apply(lambda x: x['w_name'] if x['player1'] == x['l_name'] else x['l_name'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Constructs and adds player1 and player2 columns for everything to the dataframe\n",
    "# instead of just having Winner/Loser associated statistics\n",
    "for i, column_name in enumerate(p1_columns):\n",
    "    atp_all_matches[column_name] = atp_all_matches.apply(lambda x: x[pw_columns[i]] \\\n",
    "                                                         if (x['player1'] == x['w_name']) else x[pl_columns[i]], axis = 1)\n",
    "for i, column_name in enumerate(p2_columns):\n",
    "    atp_all_matches[column_name] = atp_all_matches.apply(lambda x: x[pw_columns[i]] \\\n",
    "                                                         if (x['player2'] == x['w_name']) else x[pl_columns[i]], axis = 1)\n",
    "\n",
    "    atp_all_matches['player1_wins'] = atp_all_matches.apply(lambda x: 1 if (x['w_name'] == x['player1']) else 0, axis = 1)\n",
    "atp_all_matches['reversed'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "atp_all_matches.to_csv(\"interim245.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "atp_all_matches = pd.read_csv('jacobsdf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Switches Head to Head from winner-first to player1-first\n",
    "'''\n",
    "from ast import literal_eval\n",
    "atp_all_matches['h2h_all'] = atp_all_matches.apply(lambda x: literal_eval(x['h2h_all']) \\\n",
    "                                                   if x['player1_wins'] == 1 else literal_eval(x['h2h_all'])[::-1], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rating Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elo, Trueskill, Glicko\n",
    "1. http://gobase.org/studying/articles/elo/\n",
    "2. http://www.gamefaqs.com/boards/610657-dota-2/67994646\n",
    "3. https://github.com/sublee/elo/blob/master/elo.py\n",
    "4. http://stephenwan.net/thoughts/2012/10/02/elo-rating-system.html\n",
    "5. https://deltadata.wordpress.com/2014/01/11/glicko-2-for-tennis-part-2-the-model/comment-page-1/#comment-192\n",
    "6. https://github.com/sublee/glicko2/blob/master/glicko2.py\n",
    "7. http://trueskill.org/\n",
    "8. https://pypi.python.org/pypi/trueskill\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "''' \n",
    "Make a dict of players with elo, glicko2, and trueskill ratings\n",
    "'''\n",
    "env = glicko2.Glicko2(tau=0.7)\n",
    "base_rating = env.create_rating()\n",
    "players_list = np.union1d(atp_all_matches.w_name.values, atp_all_matches.l_name.values)\n",
    "\n",
    "players_elo = dict(zip(list(set(players_list)), [elo.Rating()] *len(players_list)))\n",
    "players_glicko = dict(zip(list(set(players_list)), [base_rating] *len(players_list)))\n",
    "players_ts = dict(zip(list(set(players_list)), [trueskill.Rating()] *len(players_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the correct Elo, Trueskill, and Glicko ratings at each match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Creates columns for the ratings and updates the dicts with the info.\n",
    "Indexing inside just one dict wasn't working, hence the three dicts.\n",
    "'''\n",
    "mean_elo, diff_elo = [], []\n",
    "mean_glicko2, diff_glicko2 = [], []\n",
    "mean_ts, diff_ts = [], []\n",
    "\n",
    "# dictionary keyed by top 5 players\n",
    "top5 = ['Novak Djokovic', 'Andy Murray', 'Roger Federer', 'Stan Wawrinka', 'Rafael Nadal']\n",
    "top5elo = {'Novak Djokovic':[], 'Andy Murray':[], 'Roger Federer':[], 'Stan Wawrinka':[], 'Rafael Nadal':[]}\n",
    "top5glicko = {'Novak Djokovic':[], 'Andy Murray':[], 'Roger Federer':[], 'Stan Wawrinka':[], 'Rafael Nadal':[]}\n",
    "top5ts = {'Novak Djokovic':[], 'Andy Murray':[], 'Roger Federer':[], 'Stan Wawrinka':[], 'Rafael Nadal':[]}\n",
    "\n",
    "\n",
    "for i, row in atp_all_matches.iterrows():\n",
    "    # Find Rating in dictionary\n",
    "    elo1 = players_elo[str(row['player1'])]\n",
    "    elo2 = players_elo[str(row['player2'])]\n",
    "    ts1 = players_ts[row['player1']]\n",
    "    ts2 = players_ts[row['player2']]\n",
    "    glickop1 = players_glicko[row['player1']]\n",
    "    glickop2 = players_glicko[row['player2']]\n",
    "    \n",
    "    # Append Rating to List for adding to DataFrame\n",
    "    mean_elo.append((elo1.value + elo2.value)/2)\n",
    "    diff_elo.append(float(elo1.value) - float(elo2.value))\n",
    "    mean_ts.append((ts1.mu + ts2.mu)/2)\n",
    "    diff_ts.append(float(ts1.mu) - float(ts2.mu))\n",
    "    mean_glicko2.append((glickop1.mu + glickop2.mu)/2)\n",
    "    diff_glicko2.append(float(glickop1.mu) - float(glickop2.mu))\n",
    "\n",
    "    # Calculate the new rating after match result\n",
    "    if row['player1_wins'] == 1:\n",
    "        new_elo1, new_elo2 = elo.rate_1vs1(elo1,elo2)\n",
    "        new_glickop1, new_glickop2 = glicko2.Glicko2.rate_1vs1(env, glickop1, glickop2)\n",
    "        new_ts1, new_ts2 = trueskill.rate_1vs1(ts1,ts2)\n",
    "    else:\n",
    "        new_elo2, new_elo1 = elo.rate_1vs1(elo2, elo1)\n",
    "        new_glickop2, new_glickop1 = glicko2.Glicko2.rate_1vs1(env, glickop1, glickop2)\n",
    "        new_ts2, new_ts1 = trueskill.rate_1vs1(ts1,ts2)\n",
    "        \n",
    "    # Update the new ratings in dictionary\n",
    "    players_elo[str(row['player1'])] = elo.Rating(new_elo1)\n",
    "    players_elo[str(row['player2'])] = elo.Rating(new_elo2)\n",
    "    players_glicko[str(row['player1'])] = new_glickop1\n",
    "    players_glicko[str(row['player2'])] = new_glickop2\n",
    "    players_ts[str(row['player1'])] = trueskill.Rating(new_ts1)\n",
    "    players_ts[str(row['player2'])] = trueskill.Rating(new_ts2)\n",
    "    \n",
    "    # add to ratings to time series for plotting\n",
    "    if row['player1'] in top5:\n",
    "        top5elo[row['player1']].append(new_elo1)\n",
    "        top5glicko[row['player1']].append(new_glickop1.mu)\n",
    "        top5ts[row['player1']].append(new_ts1)\n",
    "    if row['player2'] in top5:\n",
    "        top5elo[row['player2']].append(new_elo2)\n",
    "        top5glicko[row['player2']].append(new_glickop2.mu)\n",
    "        top5ts[row['player2']].append(new_ts2)\n",
    "\n",
    "#Add columns\n",
    "atp_all_matches['mean_elo'] = pd.Series(mean_elo, index = atp_all_matches.index)\n",
    "atp_all_matches['diff_elo'] = pd.Series(diff_elo, index = atp_all_matches.index)\n",
    "atp_all_matches['mean_glicko2'] = pd.Series(mean_glicko2, index = atp_all_matches.index)\n",
    "atp_all_matches['diff_glicko2'] = pd.Series(diff_glicko2, index = atp_all_matches.index)\n",
    "atp_all_matches['mean_ts'] = pd.Series(mean_ts, index = atp_all_matches.index)\n",
    "atp_all_matches['diff_ts'] = pd.Series(diff_ts, index = atp_all_matches.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model: Higher Ranked Player Wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size = int(atp_all_matches.shape[0]/2)\n",
    "print size\n",
    "baseline = atp_all_matches[1:size]\n",
    "len(baseline[baseline[\"diff_rank\"] <= 0])/len(baseline.diff_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Make a dict of players with elo, glicko2, and trueskill ratings'''\n",
    "env = glicko2.Glicko2(tau=0.7)\n",
    "x = env.create_rating()\n",
    "players_list = np.union1d(baseline.w_name.values, baseline.l_name.values)\n",
    "\n",
    "players_elo = dict(zip(list(set(players_list)), [elo.Rating()] *len(players_list)))\n",
    "players_glicko = dict(zip(list(set(players_list)), [x] *len(players_list)))\n",
    "players_ts = dict(zip(list(set(players_list)), [trueskill.Rating()] *len(players_list)))\n",
    "\n",
    "mean_elo, diff_elo = [], []\n",
    "mean_glicko2, diff_glicko2 = [], []\n",
    "mean_ts, diff_ts = [], []\n",
    "\n",
    "# dictionary keyed by top 5 players\n",
    "top5 = ['Novak Djokovic', 'Andy Murray', 'Roger Federer', 'Stan Wawrinka', 'Rafael Nadal']\n",
    "top5elo = {'Novak Djokovic':[], 'Andy Murray':[], 'Roger Federer':[], 'Stan Wawrinka':[], 'Rafael Nadal':[]}\n",
    "top5glicko = {'Novak Djokovic':[], 'Andy Murray':[], 'Roger Federer':[], 'Stan Wawrinka':[], 'Rafael Nadal':[]}\n",
    "top5ts = {'Novak Djokovic':[], 'Andy Murray':[], 'Roger Federer':[], 'Stan Wawrinka':[], 'Rafael Nadal':[]}\n",
    "\n",
    "for i, row in baseline.iterrows():\n",
    "    # Find Rating in dictionary\n",
    "    elo1 = players_elo[str(row['player1'])]\n",
    "    elo2 = players_elo[str(row['player2'])]\n",
    "    ts1 = players_ts[row['player1']]\n",
    "    ts2 = players_ts[row['player2']]\n",
    "    glickop1 = players_glicko[row['player1']]\n",
    "    glickop2 = players_glicko[row['player2']]\n",
    "    \n",
    "    # Append Rating to List for adding to DataFrame\n",
    "    mean_elo.append((elo1.value + elo2.value)/2)\n",
    "    diff_elo.append(float(elo1.value) - float(elo2.value))\n",
    "    mean_ts.append((ts1.mu + ts2.mu)/2)\n",
    "    diff_ts.append(float(ts1.mu) - float(ts2.mu))\n",
    "    mean_glicko2.append((glickop1.mu + glickop2.mu)/2)\n",
    "    diff_glicko2.append(float(glickop1.mu) - float(glickop2.mu))\n",
    "\n",
    "    # Calculate the new rating after match result\n",
    "    if row['player1_wins'] == 1:\n",
    "        new_elo1, new_elo2 = elo.rate_1vs1(elo1,elo2)\n",
    "        new_glickop1, new_glickop2 = glicko2.Glicko2.rate_1vs1(env, glickop1, glickop2)\n",
    "        new_ts1, new_ts2 = trueskill.rate_1vs1(ts1,ts2)\n",
    "    else:\n",
    "        new_elo2, new_elo1 = elo.rate_1vs1(elo2, elo1)\n",
    "        new_glickop2, new_glickop1 = glicko2.Glicko2.rate_1vs1(env, glickop1, glickop2)\n",
    "        new_ts2, new_ts1 = trueskill.rate_1vs1(ts1,ts2)\n",
    "        \n",
    "    # Update the new Elo ratings in dictionary\n",
    "    players_elo[str(row['player1'])] = elo.Rating(new_elo1)\n",
    "    players_elo[str(row['player2'])] = elo.Rating(new_elo2)\n",
    "    players_glicko[str(row['player1'])] = new_glickop1\n",
    "    players_glicko[str(row['player2'])] = new_glickop2\n",
    "    players_ts[str(row['player1'])] = trueskill.Rating(new_ts1)\n",
    "    players_ts[str(row['player2'])] = trueskill.Rating(new_ts2)\n",
    "    \n",
    "    # add to ratings to time series for plotting\n",
    "    if row['player1'] in top5:\n",
    "        top5elo[row['player1']].append(new_elo1)\n",
    "        top5glicko[row['player1']].append(new_glickop1.mu)\n",
    "        top5ts[row['player1']].append(new_ts1)\n",
    "    if row['player2'] in top5:\n",
    "        top5elo[row['player2']].append(new_elo2)\n",
    "        top5glicko[row['player2']].append(new_glickop2.mu)\n",
    "        top5ts[row['player2']].append(new_ts2)\n",
    "        \n",
    "#Add columns\n",
    "baseline['mean_elo'] = pd.Series(mean_elo, index = baseline.index)\n",
    "baseline['diff_elo'] = pd.Series(diff_elo, index = baseline.index)\n",
    "baseline['mean_glicko2'] = pd.Series(mean_glicko2, index = baseline.index)\n",
    "baseline['diff_glicko2'] = pd.Series(diff_glicko2, index = baseline.index)\n",
    "baseline['mean_ts'] = pd.Series(mean_ts, index = baseline.index)\n",
    "baseline['diff_ts'] = pd.Series(diff_ts, index = baseline.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(top5elo[top5[0]], color='blue', alpha=0.5)\n",
    "plt.plot(top5elo[top5[1]], color='red',  alpha=0.5)\n",
    "plt.plot(top5elo[top5[2]], color='green',  alpha=0.5)\n",
    "plt.plot(top5elo[top5[3]], color='purple',  alpha=0.5)\n",
    "plt.plot(top5elo[top5[4]], color='black',  alpha=0.5)\n",
    "plt.title('Top 5 Players - Elo')\n",
    "plt.legend(top5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(top5glicko[top5[0]], color='blue', alpha=0.5)\n",
    "plt.plot(top5glicko[top5[1]], color='red',  alpha=0.5)\n",
    "plt.plot(top5glicko[top5[2]], color='green',  alpha=0.5)\n",
    "plt.plot(top5glicko[top5[3]], color='purple',  alpha=0.5)\n",
    "plt.plot(top5glicko[top5[4]], color='black',  alpha=0.5)\n",
    "plt.title('Top 5 Players - Glicko')\n",
    "plt.legend(top5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(top5ts[top5[0]], color='blue', alpha=0.5)\n",
    "plt.plot(top5ts[top5[1]], color='red',  alpha=0.5)\n",
    "plt.plot(top5ts[top5[2]], color='green',  alpha=0.5)\n",
    "plt.plot(top5ts[top5[3]], color='purple',  alpha=0.5)\n",
    "plt.plot(top5ts[top5[4]], color='black',  alpha=0.5)\n",
    "plt.title('Top 5 Players - Trueskill')\n",
    "plt.legend(top5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "atp_all_matches_backup = atp_all_matches.copy()\n",
    "# should be (50963, 91)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creates Duplicate Rows to have the right encoding for our response\n",
    "See discussion here: http://stats.stackexchange.com/questions/11800/how-should-we-convert-sports-results-data-to-perform-a-valid-logistical-regressi.  Necessary for symmetry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CAN ONLY BE RUN ONCE\n",
    "duplicate_matches = atp_all_matches.copy()\n",
    "duplicate_matches['reversed'] = 1\n",
    "duplicate_matches['diff_elo'] = -duplicate_matches.diff_elo.values\n",
    "duplicate_matches['diff_glicko2'] = -duplicate_matches.diff_glicko2.values\n",
    "duplicate_matches['diff_ts'] = -duplicate_matches.diff_ts.values\n",
    "duplicate_matches['diff_rank'] = -duplicate_matches.diff_rank\n",
    "duplicate_matches['h2h_all_diff'] = -duplicate_matches.h2h_all_diff\n",
    "duplicate_matches['h2h_recent_diff'] = -duplicate_matches.h2h_recent_diff\n",
    "duplicate_matches['player1_wins'] = 0\n",
    "\n",
    "frame = [atp_all_matches, duplicate_matches]\n",
    "combined = pd.concat(frame, ignore_index = True)\n",
    "atp_all_matches = combined\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our predictor columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mean_elo', 'diff_elo', 'mean_rank', 'diff_rank', 'h2h_all_diff', 'h2h_recent_diff', 'p1_top10', 'p2_top10', 'p1_h2h_prop', 'p2_h2h_prop', 'h2h_all_count']\n"
     ]
    }
   ],
   "source": [
    "# create list of feature columns\n",
    "cols = ['mean_elo', 'diff_elo', 'mean_ts', 'diff_ts', 'mean_glicko2', 'diff_glicko2', 'mean_rank', 'diff_rank', 'h2h_all_diff', 'h2h_recent_diff',\n",
    "       'p1_top10', 'p2_top10', 'p1_h2h_prop', 'p2_h2h_prop', 'h2h_all_count']\n",
    "others = cols[6:]\n",
    "\n",
    "eloCols = cols[0:2] + others\n",
    "tsCols = cols[2:4] + others\n",
    "glickoCols = cols[4:6] + others\n",
    "\n",
    "print eloCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "atp_all_matches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "atp_all_matches.to_csv('jacobsdf.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Be sure to run the code in \"Important Functions\" in the Last Cell of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation using each of the rating systems separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "itrain, itest = train_test_split(xrange(atp_all_matches.shape[0]), train_size=0.7)\n",
    "\n",
    "mask=np.ones(atp_all_matches.shape[0], dtype='int')\n",
    "mask[itrain]=1\n",
    "mask[itest]=0\n",
    "mask = (mask==1)\n",
    "\n",
    "Xmatrix=atp_all_matches[cols]\n",
    "yresp=atp_all_matches['player1_wins']\n",
    "\n",
    "Xtrain=Xmatrix[mask]\n",
    "Xtest=Xmatrix[~mask]\n",
    "ytrain=yresp[mask]\n",
    "ytest=yresp[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using mask\n",
      "############# based on standard predict ################\n",
      "Accuracy on training data: 0.65\n",
      "Accuracy on test data:     0.65\n",
      "[[10017  5286]\n",
      " [ 5502  9773]]\n",
      "########################################################\n",
      "using mask\n",
      "############# based on standard predict ################\n",
      "Accuracy on training data: 0.65\n",
      "Accuracy on test data:     0.65\n",
      "[[10012  5291]\n",
      " [ 5412  9863]]\n",
      "########################################################\n",
      "using mask\n",
      "############# based on standard predict ################\n",
      "Accuracy on training data: 0.66\n",
      "Accuracy on test data:     0.66\n",
      "[[10084  5219]\n",
      " [ 5286  9989]]\n",
      "########################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dionhagan/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:16: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ -1.23704221e-07,   4.30105708e-03]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clflogELO, Xtrain, ytrain, Xtest, ytest = do_classify(clf=linear_model.LogisticRegression(fit_intercept = False),\\\n",
    "                                                   parameters = None, \\\n",
    "                                                   indf=atp_all_matches, featurenames=eloCols, targetname = 'player1_wins', \\\n",
    "                                                   target1val = 1, mask = mask)\n",
    "\n",
    "clflogTS, Xtrain1, ytrain1, Xtest1, ytest1 = do_classify(clf=linear_model.LogisticRegression(fit_intercept = False),\\\n",
    "                                                   parameters = None, \\\n",
    "                                                   indf=atp_all_matches, featurenames=tsCols, targetname = 'player1_wins', \\\n",
    "                                                   target1val = 1, mask = mask)\n",
    "\n",
    "clflogGLICKO, Xtrain2, ytrain2, Xtest2, ytest2 = do_classify(clf=linear_model.LogisticRegression(fit_intercept = False),\\\n",
    "                                                   parameters = None, \\\n",
    "                                                   indf=atp_all_matches, featurenames=glickoCols, targetname = 'player1_wins', \\\n",
    "                                                   target1val = 1, mask = mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remark\n",
    "\n",
    "As we can see, the outcomes are fairly similar, but perhaps they are making different predictions on different matches!  If this is the case, then we could use an ensemble method in order to capture that.  To this end, we continue to stacked regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Stacked Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "itrain, intermediate_set = train_test_split(xrange(atp_all_matches.shape[0]), train_size=0.6, test_size=0.4)\n",
    "icv, itest = train_test_split(intermediate_set, train_size=0.5, test_size=0.5)\n",
    "\n",
    "mask=np.ones(atp_all_matches.shape[0], dtype='int')\n",
    "mask[itrain]= 2\n",
    "mask[icv] = 1\n",
    "mask[itest]=0\n",
    "cols = ['mean_elo', 'diff_elo', 'mean_ts', 'diff_ts', 'mean_glicko2', 'diff_glicko2']\n",
    "\n",
    "Xmatrix=atp_all_matches[cols]\n",
    "yresp=atp_all_matches['player1_wins']\n",
    "\n",
    "Xtrain=Xmatrix[(mask == 2)]\n",
    "Xcv = Xmatrix[(mask == 1)]\n",
    "Xtest=Xmatrix[mask == 0]\n",
    "ytrain=yresp[(mask == 2)]\n",
    "ycv = yresp[(mask == 1)]\n",
    "ytest=yresp[(mask == 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit each individual rating model separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# based on ELO COEFFS ################\n",
      "Accuracy on training data: 0.65\n",
      "Accuracy on cv data:     0.65\n",
      "[[6578 3637]\n",
      " [3487 6683]]\n",
      "########################################################\n",
      "[[  5.12014028e-06   6.74210883e-03]]\n",
      "############# based on TS COEFFS ################\n",
      "Accuracy on training data: 0.65\n",
      "Accuracy on cv data:     0.66\n",
      "[[6651 3564]\n",
      " [3468 6702]]\n",
      "########################################################\n",
      "[[  5.12014028e-06   6.74210883e-03]]\n",
      "############# based on GLICKO2 COEFFS ################\n",
      "Accuracy on training data: 0.66\n",
      "Accuracy on cv data:     0.66\n",
      "[[6736 3479]\n",
      " [3392 6778]]\n",
      "########################################################\n",
      "[[4.07954597664712e-06, 0.004272655905636718]]\n"
     ]
    }
   ],
   "source": [
    "Ztrain = Xtrain[['mean_elo', 'diff_elo']]\n",
    "Zcv = Xcv[['mean_elo', 'diff_elo']]\n",
    "lm2cv = linear_model.LogisticRegression(fit_intercept = False).fit(Ztrain, ytrain)\n",
    "lm2test =  linear_model.LogisticRegression(fit_intercept = False).fit(Ztrain, ytrain)\n",
    "\n",
    "y = lm2cv.coef_.tolist()\n",
    "x2 = pd.DataFrame(zip(Ztrain.columns, sum(y, [])), columns = ['features', 'estimatedCoefficients'])\n",
    "\n",
    "training_accuracy = lm2cv.score(Ztrain, ytrain)\n",
    "cv_accuracy = lm2cv.score(Zcv, ycv)\n",
    "print \"############# based on ELO COEFFS ################\"\n",
    "print \"Accuracy on training data: %0.2f\" % (training_accuracy)\n",
    "print \"Accuracy on cv data:     %0.2f\" % (cv_accuracy)\n",
    "print confusion_matrix(ycv, lm2cv.predict(Zcv))\n",
    "print \"########################################################\"\n",
    "print lm2cv.coef_\n",
    "\n",
    "Wtrain = Xtrain[['mean_ts', 'diff_ts']]\n",
    "Wcv = Xcv[['mean_ts', 'diff_ts']]\n",
    "lm3cv = linear_model.LogisticRegression(fit_intercept = False).fit(Ztrain, ytrain)\n",
    "lm3test = linear_model.LogisticRegression(fit_intercept = False).fit(Ztrain, ytrain)\n",
    "y = lm3cv.coef_.tolist()\n",
    "x3 = pd.DataFrame(zip(Wtrain.columns, sum(y, [])), columns = ['features', 'estimatedCoefficients'])\n",
    "\n",
    "training_accuracy = lm3cv.score(Wtrain, ytrain)\n",
    "cv_accuracy = lm3cv.score(Wcv, ycv)\n",
    "print \"############# based on TS COEFFS ################\"\n",
    "print \"Accuracy on training data: %0.2f\" % (training_accuracy)\n",
    "print \"Accuracy on cv data:     %0.2f\" % (cv_accuracy)\n",
    "print confusion_matrix(ycv, lm3cv.predict(Wcv))\n",
    "print \"########################################################\"\n",
    "print lm3cv.coef_\n",
    "\n",
    "Gtrain = Xtrain[['mean_glicko2', 'diff_glicko2']]\n",
    "Gcv = Xcv[['mean_glicko2', 'diff_glicko2']]\n",
    "lm4cv = linear_model.LogisticRegression(fit_intercept = False).fit(Gtrain, ytrain)\n",
    "lm4test = linear_model.LogisticRegression(fit_intercept = False).fit(Gtrain, ytrain)\n",
    "y = lm4cv.coef_.tolist()\n",
    "x4 = pd.DataFrame(zip(Gtrain.columns, sum(y, [])), columns = ['features', 'estimatedCoefficients'])\n",
    "\n",
    "training_accuracy = lm4cv.score(Gtrain, ytrain)\n",
    "cv_accuracy = lm4cv.score(Gcv, ycv)\n",
    "print \"############# based on GLICKO2 COEFFS ################\"\n",
    "print \"Accuracy on training data: %0.2f\" % (training_accuracy)\n",
    "print \"Accuracy on cv data:     %0.2f\" % (cv_accuracy)\n",
    "print confusion_matrix(ycv, lm4cv.predict(Gcv))\n",
    "print \"########################################################\"\n",
    "print y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regress on their predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.157482958674\n",
      "[[6722 3574]\n",
      " [3406 6684]]\n",
      "0.657608162464\n"
     ]
    }
   ],
   "source": [
    "predictions = {}\n",
    "predictions_valid={}\n",
    "predictions_valid['elo'] = lm2cv.predict_proba(Xcv[cols[0:2]])[:,0]\n",
    "predictions_valid['ts'] = lm3cv.predict_proba(Xcv[cols[2:4]])[:,0]\n",
    "predictions_valid['glicko'] = lm4cv.predict_proba(Xcv[cols[4:6]])[:,0]\n",
    "\n",
    "dfensemble=pd.DataFrame.from_dict({'elo':predictions_valid['elo'],\n",
    "                                   'ts':predictions_valid['ts'], \n",
    "                                   'glicko':predictions_valid['glicko'], 'y': ycv})\n",
    "\n",
    "valreg = linear_model.LinearRegression()\n",
    "valreg.fit(dfensemble[['elo','ts', 'glicko']], dfensemble['y'])\n",
    "\n",
    "predictions['elo'] = lm2test.predict_proba(Xtest[cols[0:2]])[:,0]\n",
    "predictions['ts'] = lm2test.predict_proba(Xtest[cols[2:4]])[:,0]\n",
    "predictions['glicko'] = lm2test.predict_proba(Xtest[cols[4:6]])[:,0]\n",
    "\n",
    "\n",
    "dfensembletest = pd.DataFrame.from_dict({'elo':predictions['elo'],\n",
    "                                   'ts':predictions['ts'], \n",
    "                                   'glicko':predictions['glicko'], 'y':ytest})\n",
    "epreds = valreg.predict(dfensembletest[['elo','ts', 'glicko']])\n",
    "testactual = dfensembletest['y'].values\n",
    "\n",
    "training_accuracy = valreg.score(dfensemble[['elo', 'ts', 'glicko']], dfensemble['y'])\n",
    "print training_accuracy\n",
    "predicted_outcome = [1 if x > 0.5 else 0 for x in epreds]\n",
    "confuse = confusion_matrix(testactual, predicted_outcome)\n",
    "print confuse\n",
    "testing_accuracy = (confuse[0,0] + confuse[1,1])/(confuse[0,0]+confuse[0,1]+confuse[1,0]+confuse[1,1])\n",
    "print testing_accuracy\n",
    "#print (6694+6725)/(6660+6725 + 3493+3507)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with sns.color_palette(\"hls\", 3):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    ax = make_roc(\"adaboost\", clfAda, ytesta, Xtesta, labe=200, skip=50)\n",
    "    ax = make_roc(\"naive-bayes\", clfNB, yte, Xte, labe=200, skip=50, ax = ax, proba = True)\n",
    "    ax = make_roc(\"clflogELO\", clflogELO, ytest, Xtest, labe=200, skip=50, ax = ax, proba = True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.65\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[10104,  5452],\n",
       "       [ 5300,  9722]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=atp_all_matches[cols].values\n",
    "y=atp_all_matches.player1_wins.values\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clfNB=GaussianNB()\n",
    "itr,ite=train_test_split(xrange(atp_all_matches.shape[0]), train_size=0.7)\n",
    "Xtr=X[itr]\n",
    "Xte=X[ite]\n",
    "ytr=y[itr]\n",
    "yte=y[ite]\n",
    "clfNB.fit(Xtr,ytr)\n",
    "test_acc = 1 - float((yte != clfNB.predict(Xte)).sum())/yte.shape[0]\n",
    "print \"Testing Accuracy: %.02f\" % test_acc\n",
    "confusion_matrix(clfNB.predict(Xte),yte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'do_classify' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6788abe8543e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"n_estimators\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m clfAda, Xtraina, ytraina, Xtesta, ytesta = do_classify(clfAda, parameters, \n\u001b[0m\u001b[1;32m      8\u001b[0m                                                        \u001b[0mindf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0matp_all_matches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                                                        \u001b[0mfeaturenames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'player1_wins'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget1val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'do_classify' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "clfAda = AdaBoostClassifier()\n",
    "\n",
    "parameters = {\"n_estimators\": range(10, 60)}\n",
    "clfAda, Xtraina, ytraina, Xtesta, ytesta = do_classify(clfAda, parameters, \n",
    "                                                       indf=atp_all_matches, \n",
    "                                                       featurenames=cols, targetname='player1_wins', target1val = 1, mask=mask, \n",
    "                                                       score_func='f1', n_jobs=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importance_list = clfAda.feature_importances_\n",
    "name_list = cols\n",
    "importance_list, name_list = zip(*sorted(zip(importance_list, name_list)))\n",
    "plt.barh(range(len(name_list)),importance_list,align='center')\n",
    "plt.yticks(range(len(name_list)),name_list)\n",
    "plt.xlabel('Relative Importance in the Adaboost Classifier')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Relative importance of Each Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dionhagan/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:16: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-ae1b3de3ada8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                                                        \u001b[0mindf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0matp_all_matches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                                        \u001b[0mfeaturenames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'player1_wins'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget1val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                                                        score_func='f1', n_jobs=4)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-57-cdc362a6a5ef>\u001b[0m in \u001b[0;36mdo_classify\u001b[0;34m(clf, parameters, indf, featurenames, targetname, target1val, mask, reuse_split, score_func, n_folds, n_jobs)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreuse_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Xtrain'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Xtest'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ytrain'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ytest'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_optimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_folds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mclf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mtraining_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-cdc362a6a5ef>\u001b[0m in \u001b[0;36mcv_optimize\u001b[0;34m(clf, parameters, X, y, n_jobs, n_folds, score_func)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_folds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"BEST\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_scores_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dionhagan/anaconda/lib/python2.7/site-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         \"\"\"\n\u001b[0;32m--> 732\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dionhagan/anaconda/lib/python2.7/site-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[1;32m    503\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m                                     error_score=self.error_score)\n\u001b[0;32m--> 505\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m                 for train, test in cv)\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dionhagan/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    664\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 666\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    667\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dionhagan/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    530\u001b[0m                             \u001b[0;31m# We can now allow subprocesses again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m                             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__JOBLIB_SPAWNED_PARALLEL__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m                     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransportableException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m                         \u001b[0;31m# Capture exception to add information on the local\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clfGB = GradientBoostingClassifier()\n",
    "\n",
    "parameters = {\"n_estimators\": range(30, 60), \"max_depth\": [1, 2, 3, 4, 5]}\n",
    "clfGB, Xtrain, ytrain, Xtest, ytest = do_classify(clfGB, parameters, \n",
    "                                                       indf=atp_all_matches, \n",
    "                                                       featurenames=cols, targetname='player1_wins', target1val = 1, mask=mask, \n",
    "                                                       score_func='f1', n_jobs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Point-by-Point Probability Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'atp_all_matches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-531061e7f0f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0matp_all_matches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'atp_all_matches' is not defined"
     ]
    }
   ],
   "source": [
    "# https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter6_Priorities/Chapter6.ipynb\n",
    "\n",
    "# Subset the dataframe only for rows that we have point-by-point data on\n",
    "#atp_point_by_point_matches = atp_all_matches.iloc['pbp' == None]\n",
    "\n",
    "# Establish prior probabilities for each given the players involved, or perhaps based on the other models\n",
    "\n",
    "# Is Markov chain approach more appropriate?\n",
    "\n",
    "def decision_function(x):\n",
    "    if x['pbp_sim_prob_w'] > 0.55:\n",
    "        return x['w_name']\n",
    "    elif x['pbp_sim_prob_w'] < 0.45:\n",
    "        return x['l_name']\n",
    "    else:\n",
    "        return 'Close'  \n",
    "    \n",
    "atp_all_matches_pbp = atp_all_matches.copy()\n",
    "atp_all_matches_pbp = atp_all_matches_pbp[reversed == 0]\n",
    "atp_all_matches_pbp['pbp_sim_prob_w'].apply(lambda x: n_match_simulator(x['w_name'], x['loser_name'], x['match_year'], x['Surface'], 1000), \\\n",
    "                                                  axis = 1)\n",
    "atp_all_matches_pbp['pbp_sim_pred_winner'] == atp_all_matches_pbp['pbp_sim_prob_winner'].apply(decision_function, axis = 1)\n",
    "sum(atp_all_matches_pbp['pbp_sim_pred_winner'] == atp_all_matches_pbp['w_name'])/atp_all_matches_pbp.shape[0]\n",
    "sum(atp_all_matches_pbp['pbp_sim_pred_winner'] == 'Close')/atp_all_matches_pbp.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTANT FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cv_optimize(clf, parameters, X, y, n_jobs=1, n_folds=5, score_func=None):\n",
    "    if score_func:\n",
    "        gs = GridSearchCV(clf, param_grid=parameters, cv=n_folds, n_jobs=n_jobs, scoring=score_func)\n",
    "    else:\n",
    "        gs = GridSearchCV(clf, param_grid=parameters, n_jobs=n_jobs, cv=n_folds)\n",
    "    gs.fit(X, y)\n",
    "    print \"BEST\", gs.best_params_, gs.best_score_, gs.grid_scores_\n",
    "    best = gs.best_estimator_\n",
    "    return best\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def do_classify(clf, parameters, indf, featurenames, targetname, target1val, mask=None, reuse_split=None, score_func=None, n_folds=5, n_jobs=1):\n",
    "    subdf=indf[featurenames]\n",
    "    X=subdf.values\n",
    "    y=(indf[targetname].values==target1val)*1\n",
    "    if mask !=None:\n",
    "        print \"using mask\"\n",
    "        Xtrain, Xtest, ytrain, ytest = X[mask], X[~mask], y[mask], y[~mask]\n",
    "    if reuse_split !=None:\n",
    "        print \"using reuse split\"\n",
    "        Xtrain, Xtest, ytrain, ytest = reuse_split['Xtrain'], reuse_split['Xtest'], reuse_split['ytrain'], reuse_split['ytest']\n",
    "    if parameters:\n",
    "        clf = cv_optimize(clf, parameters, Xtrain, ytrain, n_jobs=n_jobs, n_folds=n_folds, score_func=score_func)\n",
    "    clf=clf.fit(Xtrain, ytrain)\n",
    "    training_accuracy = clf.score(Xtrain, ytrain)\n",
    "    test_accuracy = clf.score(Xtest, ytest)\n",
    "    print \"############# based on standard predict ################\"\n",
    "    print \"Accuracy on training data: %0.2f\" % (training_accuracy)\n",
    "    print \"Accuracy on test data:     %0.2f\" % (test_accuracy)\n",
    "    print confusion_matrix(ytest, clf.predict(Xtest))\n",
    "    print \"########################################################\"\n",
    "    return clf, Xtrain, ytrain, Xtest, ytest\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "def make_roc(name, clf, ytest, xtest, ax=None, labe=5, proba=True, skip=0):\n",
    "    initial=False\n",
    "    if not ax:\n",
    "        ax=plt.gca()\n",
    "        initial=True\n",
    "    if proba:#for stuff like logistic regression\n",
    "        fpr, tpr, thresholds=roc_curve(ytest, clf.predict_proba(xtest)[:,1])\n",
    "    else:#for stuff like SVM\n",
    "        fpr, tpr, thresholds=roc_curve(ytest, clf.decision_function(xtest))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    if skip:\n",
    "        l=fpr.shape[0]\n",
    "        ax.plot(fpr[0:l:skip], tpr[0:l:skip], '.-', alpha=0.3, label='ROC curve for %s (area = %0.2f)' % (name, roc_auc))\n",
    "    else:\n",
    "        ax.plot(fpr, tpr, '.-', alpha=0.3, label='ROC curve for %s (area = %0.2f)' % (name, roc_auc))\n",
    "    label_kwargs = {}\n",
    "    label_kwargs['bbox'] = dict(\n",
    "        boxstyle='round,pad=0.3', alpha=0.2,\n",
    "    )\n",
    "    if labe!=None:\n",
    "        for k in xrange(0, fpr.shape[0],labe):\n",
    "            #from https://gist.github.com/podshumok/c1d1c9394335d86255b8\n",
    "            threshold = str(np.round(thresholds[k], 2))\n",
    "            ax.annotate(threshold, (fpr[k], tpr[k]), **label_kwargs)\n",
    "    if initial:\n",
    "        ax.plot([0, 1], [0, 1], 'k--')\n",
    "        ax.set_xlim([0.0, 1.0])\n",
    "        ax.set_ylim([0.0, 1.05])\n",
    "        ax.set_xlabel('False Positive Rate')\n",
    "        ax.set_ylabel('True Positive Rate')\n",
    "        ax.set_title('ROC')\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
